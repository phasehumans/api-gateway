{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# AIML 39"],"metadata":{"id":"ihSfS3KTJ_R7","executionInfo":{"status":"ok","timestamp":1763086710173,"user_tz":-330,"elapsed":14,"user":{"displayName":"Chaitanya","userId":"17149693889078725148"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Download necessary NLTK data files\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')\n","\n","def text_similarity(text1, text2):\n","    # Initialize lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","\n","    # Tokenize\n","    tokens1 = word_tokenize(text1)\n","    tokens2 = word_tokenize(text2)\n","\n","    # Lemmatize\n","    tokens1 = [lemmatizer.lemmatize(token.lower()) for token in tokens1 if token.isalpha()]\n","    tokens2 = [lemmatizer.lemmatize(token.lower()) for token in tokens2 if token.isalpha()]\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    tokens1 = [token for token in tokens1 if token not in stop_words]\n","    tokens2 = [token for token in tokens2 if token not in stop_words]\n","\n","    # Join tokens back into strings\n","    text1_processed = \" \".join(tokens1)\n","    text2_processed = \" \".join(tokens2)\n","\n","    # Create TF-IDF vectors\n","    vectorizer = TfidfVectorizer()\n","    vectors = vectorizer.fit_transform([text1_processed, text2_processed])\n","\n","    # âœ… Correct way to compute cosine similarity\n","    similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n","\n","    return similarity\n","\n","\n","# Example usage:\n","text1 = \"Natural Language Processing makes computers understand human language.\"\n","text2 = \"NLP enables machines to comprehend what people say or write.\"\n","print(\"Similarity Score:\", text_similarity(text1, text2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZjjFHr7ViOL","outputId":"7fc6e456-11d9-4a37-81f4-5a6409b9941a","executionInfo":{"status":"ok","timestamp":1763086731275,"user_tz":-330,"elapsed":21093,"user":{"displayName":"Chaitanya","userId":"17149693889078725148"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Similarity Score: 0.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kWf387ibKBh3"},"execution_count":null,"outputs":[]}]}