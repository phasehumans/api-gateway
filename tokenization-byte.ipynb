{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OoLMJQcYX_NE"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "print(f\"Vocabulary size: {tokenizer.n_vocab:,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9f_rYDzYK8Y",
        "outputId": "9ef1e2bb-bbcd-49d3-a530-a6d0b82a827e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 100,277 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello world\"\n",
        "tokens = tokenizer.encode(text)\n",
        "\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "print(\"\\nToken breakdown:\")\n",
        "for token in tokens:\n",
        "    print(f\"  {token} â†’ '{tokenizer.decode([token])}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft5igcNJYUvi",
        "outputId": "4531ab9d-e059-4f25-82f8-0193efdfee69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'Hello world'\n",
            "Tokens: [9906, 1917]\n",
            "Number of tokens: 2\n",
            "\n",
            "Token breakdown:\n",
            "  9906 â†’ 'Hello'\n",
            "  1917 â†’ ' world'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    \"Hello world\",\n",
        "    \"don't\",\n",
        "    \"artificial intelligence\",\n",
        "    \"I love AI\",\n",
        "    \"supercalifragilisticexpialidocious\",\n",
        "    \"ðŸš€\",\n",
        "    \"cafÃ©\",\n",
        "    \"    spaces    \",\n",
        "]\n",
        "\n",
        "print(\"How different texts get tokenized:\\n\")\n",
        "for text in examples:\n",
        "    tokens = tokenizer.encode(text)\n",
        "    print(f\"'{text}'\")\n",
        "    print(f\"  â†’ {len(tokens)} tokens: {tokens}\")\n",
        "\n",
        "    pieces = [tokenizer.decode([t]) for t in tokens]\n",
        "    print(f\"  â†’ pieces: {pieces}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3sGHkMqYiir",
        "outputId": "6d6a1e36-9396-40ed-9c3f-b0f8426d6e29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How different texts get tokenized:\n",
            "\n",
            "'Hello world'\n",
            "  â†’ 2 tokens: [9906, 1917]\n",
            "  â†’ pieces: ['Hello', ' world']\n",
            "\n",
            "'don't'\n",
            "  â†’ 2 tokens: [15357, 956]\n",
            "  â†’ pieces: ['don', \"'t\"]\n",
            "\n",
            "'artificial intelligence'\n",
            "  â†’ 3 tokens: [472, 16895, 11478]\n",
            "  â†’ pieces: ['art', 'ificial', ' intelligence']\n",
            "\n",
            "'I love AI'\n",
            "  â†’ 3 tokens: [40, 3021, 15592]\n",
            "  â†’ pieces: ['I', ' love', ' AI']\n",
            "\n",
            "'supercalifragilisticexpialidocious'\n",
            "  â†’ 11 tokens: [13066, 3035, 278, 333, 4193, 321, 4633, 4683, 532, 307, 78287]\n",
            "  â†’ pieces: ['sup', 'erc', 'al', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious']\n",
            "\n",
            "'ðŸš€'\n",
            "  â†’ 3 tokens: [9468, 248, 222]\n",
            "  â†’ pieces: ['ï¿½', 'ï¿½', 'ï¿½']\n",
            "\n",
            "'cafÃ©'\n",
            "  â†’ 2 tokens: [936, 59958]\n",
            "  â†’ pieces: ['ca', 'fÃ©']\n",
            "\n",
            "'    spaces    '\n",
            "  â†’ 3 tokens: [262, 12908, 257]\n",
            "  â†’ pieces: ['   ', ' spaces', '    ']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"my name is chaitanya\"\n",
        "\n",
        "tokens = tokenizer.encode(my_text)\n",
        "print(f\"Text: '{my_text}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "print(f\"Pieces: {[tokenizer.decode([t]) for t in tokens]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEL4nF2NYpSA",
        "outputId": "b6e0d57b-9833-4875-9533-4afac72f0063"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'my name is chaitanya'\n",
            "Tokens: [2465, 836, 374, 523, 1339, 25041]\n",
            "Number of tokens: 6\n",
            "Pieces: ['my', ' name', ' is', ' ch', 'ait', 'anya']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = {\n",
        "    \"English prose\": \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Python code\": \"def hello():\\n    print('Hello, world!')\",\n",
        "    \"JSON\": '{\"name\": \"Alice\", \"age\": 30, \"city\": \"NYC\"}',\n",
        "    \"Numbers\": \"1234567890 9876543210 1111111111\",\n",
        "    \"URL\": \"https://www.example.com/path/to/page?query=value\",\n",
        "}\n",
        "\n",
        "print(\"Token efficiency comparison:\\n\")\n",
        "for name, text in test_cases.items():\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chars = len(text)\n",
        "    ratio = chars / len(tokens)\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  {chars} chars â†’ {len(tokens)} tokens ({ratio:.1f} chars/token)\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxp6FXk7ZGyO",
        "outputId": "e7f1c5bd-889b-4115-a258-5d8d89cac488"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token efficiency comparison:\n",
            "\n",
            "English prose:\n",
            "  44 chars â†’ 10 tokens (4.4 chars/token)\n",
            "\n",
            "Python code:\n",
            "  39 chars â†’ 11 tokens (3.5 chars/token)\n",
            "\n",
            "JSON:\n",
            "  43 chars â†’ 19 tokens (2.3 chars/token)\n",
            "\n",
            "Numbers:\n",
            "  32 chars â†’ 14 tokens (2.3 chars/token)\n",
            "\n",
            "URL:\n",
            "  48 chars â†’ 11 tokens (4.4 chars/token)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"strawberry\"\n",
        "tokens = tokenizer.encode(word)\n",
        "\n",
        "print(f\"Word: '{word}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Pieces: {[tokenizer.decode([t]) for t in tokens]}\")\n",
        "print()\n",
        "print(\"The model sees these pieces, not individual letters!\")\n",
        "print(\"Counting 'r's requires looking INSIDE tokens â€” that's hard.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE-KgoBoZPM4",
        "outputId": "766efa81-0748-4d06-dc28-21d6310cba52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 'strawberry'\n",
            "Tokens: [496, 675, 15717]\n",
            "Pieces: ['str', 'aw', 'berry']\n",
            "\n",
            "The model sees these pieces, not individual letters!\n",
            "Counting 'r's requires looking INSIDE tokens â€” that's hard.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"hello\"\n",
        "tokens = tokenizer.encode(word)\n",
        "print(f\"'{word}' â†’ tokens: {[tokenizer.decode([t]) for t in tokens]}\")\n",
        "print()\n",
        "print(\"If 'hello' is ONE token, the model can't easily reverse it.\")\n",
        "print(\"It would need to decompose something it sees as atomic.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bzUyaWmZV6X",
        "outputId": "4dca1191-f8e5-43c7-df82-6a7c34e6bcc5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'hello' â†’ tokens: ['hello']\n",
            "\n",
            "If 'hello' is ONE token, the model can't easily reverse it.\n",
            "It would need to decompose something it sees as atomic.\n"
          ]
        }
      ]
    }
  ]
}