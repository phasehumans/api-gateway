### AI Overview

- what does it mean to learn?
    - building accuarate mental model to understand, predict and use
    - learning : confusion -> pattern recognition -> understanding -> prediction
- what is knowledge? what does it mean to know something?
    - mental model that represents how something works
    - understanding + structure + predictive power
        - why knowledge needs to be in structured, why not unstructure?
            - brain connect ideas, structure is req to build relationships
            - reasoning needs chaining; chain needs connection; unstructure knowledge dont have connections
            - structure : info -> understanding
    - child [2+2=4 -> memory]
    - but when child can do 3+3, and use to count obj
    - then child knows addition
    - knowing - ability to use mental model to understand, predict and act
- what is intelligence?
    - ability to observe, understand hidden patterns, predict outcomes, decide what to act/do
    - ability to build and use mental models to solve new problmes
    - not relying on just memory
- what is ai?
    - getting machines to perform tasks that req human intelligence
    - build there own understanding of world, act on it
    - figure it out
- hardcode intelligence (symbolic ai era)
    - how do we hardcode intelligence?
        - explicitly writing rules that tells machines what to do in every situation
        - if and then
        - rule based : encode expert knowledge
    - examples of rule based systems?
        - early chess engines
        - spam filters before ml
        - eliza chatbot
        - npc's in games
    - what were drawbacks that lead to ml?
        - rule based dont scale, adapt, generalize, improve over time
        - ambiguity
        - rules conflicting rules (rules about rules)
        - reality refuses to fit into rules
        - intelligence is not list of rules , it is process of learning from exp
        - stop telling machines what are rules, let them derive rules from data (rules --> patterns)
- machine learning
    - what is machine learning?
        - way to make computers learn patterns from data
        - itelligence = patterns discovery + generalization
        - adjust numbers (paramters) based on mistakes
        - learning lives in model
        - find mistakes through loss (error) and adjust parameters
        - ml dont try to be perfect, it try to be less wrong over time
    - types and examples of machine learning algorithms?
        - supervised learning - learn with answers/output
            - learning a mapping from input to output using labled(input + corrrect answer attach) examples
            - classification -> put into right bucket
                - spam/notspam, fraud/notfraud, cat/dog
                - draw boundries using past examples
                - knn
                - logistic regression
                - naive bayes
                - decision tree
                - random forest
                - svm (support vector machine)
            - regression -> predict a number
                - house price prediction, delivery time predicition
                - fn that map input ---> numbers
                - linear regression
                - polynomial regression
                - knn regression
                - decision tree regression
                - random forest regression
                - svr (support vector regression)
            - cant discover unknown patterns
            -need label data (expensive)
        - unsupervised learning - learns without answers
            - learning w/o label
            - explore data itself and discover structure, patterns
            - clustering -> discover grouping
                - grouping similar datapoints together w/o knowing groups in advance
                - k-means
                - hierarchical clustering (agglomerative and divisive)
                - dbscan
                - mean shift
                - gausssian mixture models
            - dimensionality reduction -> simplify complex data w/o losing meaning
                - represnt high dimension data into fewer and meaningful dimension
                - pca (principal component analysis)
                - t-sne
                - umap
                - autoencoders
            - anomaly detection -> spot unusual
                - far from avg
                - far from neighbors
                - sparse density
                - tiny clusters
                - isolation forest
        - reinforcement learning - learn from reward and punishment
            - learn by doing, not by being told
            - learn from consequences, not answers
            - agent, env, state, action, reward
            - value based methods -> how good is this situation
                - q-learning
            - policy based methods -> what should i do |state --> action|
            - actor-crictic methods -> one act, one judge
            - model based rl -> think before act
            - rl rewards are sparse
            - training is unstable
            - not prediction, but control
        - semi-supervised learning - little guidance, a lot of exploration (few labels + lot of data)
        - self-supervised learning - learning by creating own tasks (predict missing info) --> base of llms
    - what are important concepts in ml?
        - data
        - features - measurements, properties, signals
        - parameters - internal numbers inside model that store what model has learned from data
        - model - fn w/ parameters
        - loss fn - distance betn prediction and reality
        - learning - reducing loss fn
        - optimization - process of adjust model (fn w/ para)
        - generalization - performace on unseen data
        - overfitting - memorized noise
        - underfitting - didn't learn enough
        - bias - simplicity
        - variance - sensitivity
        - evaluation metrics - accuracy, precision, recall
        - hyperparameters -  controllers for learning rate
        - assumptions - hidden beliefs
    - fundamental diff betn ml and dl?
        - feature engineering - tell model what to look for
        - ml cant learn good representation by itself
        - in dl, models auto learn hierarchical features from raw data using layers
        - ml : input + feat-eng --> output
        - dl : input --->  layer | layer | layer | layer | ...| layers | ---> output
    - what are fundamentals of ml that are carry forward to dl and transformers?
        - core of ml -> guess > measure error > adjust > repeat
        - representation
        - model is still fn with parameters
            - what are parameters?
                - numbers that represent learning
                - learned numbers / memory of models
                - weight
                - bias
        - loss fn; goal is to reduce loss
        - optimization
        - generalization
    - what drawbacks of ml that leads to neural networks?
        - classical ml cant learn representations by itself
        - real world data is non-linear and complex
        - cant represent hierarchical concepts
        - manual feat eng
        - cant learn representation
        - neural networks for representation learning
- neural networks
    - what are neural networks?
        - learns by applying many small transformations in sequence, where is each transformations builds on prev one
        - to build models, that can learn in layers
        - neurons - tiny pattern detectors
        - layers - abstraction levels
        - parameters - learned knowledge
    - how does the nueral networks are actually trained?
        - input -> vector of numbers (image - pixel, text - token, audio - wave amplitudes)
        - forward propagation - structured guess
        - back propagation - traces responsibility backward and assigns blame proportionally
        - parameters are updated to reduce loss
        - networks dont understand concepts, it reacts to patterns
        - loop:
            - inputs go forward
            - network predicts output
            - loss is calculated
            - errors flow backward
            - parameters are adjusted
    - what is rnn?
        - invented to give nueral networks memory
        - nn that passes info from one step to next, so it can remember the past
        - hidden state is memory of nn
        - hidden state is vector of numbers, that summarizes everything seen so far
        - rnn memory is fragile and slow
    - how rnn is different from nueral network?
        - rnn is neural network
        - normal nn has no memory, treats each input independently
        - rnn feeds its previos output back into itself
        - nn : input --> |layers| --> output
        - rnn : input --> memory1 ; input + memory1 ---> memory2 ; ....
    - examples of rnn?
        - predictive keyboards
        - early google translation
    - what are limitations of rnn?
        - weak long term memory
        - training is slow
        - attention is better
    - diff betn cnn and rnn?
        - cnn for image / space
        - rnn for sequence / time
        - cnn process parallel
        - rnn process sequentail
    - why it is harder for nets to understand language over image?
        - image is spatial
        - language is sequential and contextual
        - pixels that matter are physically near to each other
        - imp words that matter, might be far apart in the sequence, long range dependency
        - language req remembering and relating info across distances
    - what are sequence models?
        - neural net treats every input has independent
        - but lang, stock price, music; each input depends upon its prev
        - so, models that are designed to handle data where order and context matter
        - process one elmt at time, maintain memory(state)
        - ex. rnn, lstm
    - does transformer is also part of sequence models?
        - yes, but use attention instead of recurrent memory
        - sequence model is not any architecture
        - any model that handle ordered data, use context
        - old soln : rnn, lstm, gru; they pass hidden state(that is what i saw in the past)
        - but this memory fades over long sequence
        - new soln : attention-based models
        - look everything at once, decide what matters and connect them directly
    - why do rnn, lstm, bert and other models fail?
        - rnn process step by step and carry hidden state(memory : compress info) forward
        - at longer seq, info loss; forget long term dependencies
        - in lstm added gates to control memory
        - but still sequential and recurrence, no full context access
        - cnn look for local neighborhoods, design for spatial patterns
        - bert is not fail, it is transformer
        - understanding but not genrational
        - bert -> gpt
        - rnn, lstm, gru all needed a mechnaism that allows direct, global, || interaction betn all elmt of seq
        - soln is attention
- attention
    - what is attention?
        - 
        - model decide where to focus
        - look back and focus on what matters right now
    - why attention is powerful than nueral nets?
    - model decide where to focus?
    - look back and focus on what matters right now?
- transformer
    - what is transformer?
    - how chatgpt works?
    - how models are trainde?
    - why predicting next word creates understanding?
    - how reasoning gets emerges in models?
    - what is foundational models?
    - what is llm?
    - what is slm?

--- 
- [LLM visualization](https://bbycroft.net/llm)